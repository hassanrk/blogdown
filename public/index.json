[{"authors":null,"categories":null,"content":"This feature can be used for publishing content such as:\n Project or software documentation Online courses Tutorials  The parent folder may be renamed, for example, to docs for project documentation or course for creating an online course.\nTo disable this feature, either delete the parent folder, or set draft = true in the front matter of all its pages.\nAfter renaming or deleting the parent folder, you may wish to update any [[menu.main]] menu links to it in the config.toml.\n","date":1536469200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536469200,"objectID":"c3224f3a64174f08aaf31e1f1d16ffd3","permalink":"/tutorial/","publishdate":"2018-09-09T00:00:00-05:00","relpermalink":"/tutorial/","section":"tutorial","summary":"This feature can be used for publishing content such as:\n Project or software documentation Online courses Tutorials  The parent folder may be renamed, for example, to docs for project documentation or course for creating an online course.\nTo disable this feature, either delete the parent folder, or set draft = true in the front matter of all its pages.\nAfter renaming or deleting the parent folder, you may wish to update any [[menu.","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":["R"],"content":"  The greatest value of a picture is when it forces us to notice what we never expected to see.\n— John Tukey\n I remember taking a class with coatless while I was still in my third year of undergraduate, completely devoid of any programming language. He struck upon me as a teacher who was ridiculously enthusiastic about R, though was passionate and careful in explaining the fundamental concepts to his students. In one of his lessons, he talked about an app called Shiny that provides an interactive Live interface with the user. Sounds cool? Well, it is. More so, it is one of the things in programming that forces interactivity between a human and a computer. Just think about the endless possibility!\nHere is a Shiny app that uses voice recognition.\nShiny builds on the idea that one can build interactive web apps straight from R. It has two main components, a user interface (UI) and a server. A server is typically defined as the backend of data retrieval and functionality that perform all the transformations, while the UI serves as the frontend or client-side where a user interacts accordingly given a set of inputs, sort of like how the customer service/service provider works.\nAs an activity, I was told to visualize data on a Shiny app and interpret the results. I carefully picked a topic and decided to do a project about gun-related deaths in the United States. I gathered a dataset from FiveThirtyEight that has data collected by the Centers for Disease Control and Prevention (CDC) on gun-related deaths from the year 2012 to 2014. Fast forward the data wrangling, we are ready to build a Shiny app.\nLet’s start by building the UI template for our Shiny app. (final product)\nStep 1: Define a UI. library(shiny) ui = shinyUI( fluidPage( titlePanel(\u0026quot;Gun-Related Deaths in the United States (2012-2014)\u0026quot;), #Title sidebarLayout( sidebarPanel( h1(\u0026quot;This is my input\u0026quot;) ), mainPanel(\u0026quot;This is my output\u0026quot;) ) # close: sidebarLayout() ) # close: fluidPage() ) # close: shinyUI() server = function(input, output){} shinyApp(ui, server)  This is the raw backbone behind our Shiny app. For our app, we want to make four different types of visualization, so ideally we would want four different tabs. To achieve that, we would use tabsetPanel() and within it, four tabPanel()’s, in which we would include the same structure as we did previously. I shortened “Explanatory Data Analysis” to just EDA.\nlibrary(shiny) ui = fluidPage( titlePanel(\u0026quot;Gun-Related Deaths in the United States (2012-2014)\u0026quot;), tabsetPanel( tabPanel(\u0026quot;Quantitative EDA\u0026quot;, fluid = TRUE, #tab 1 sidebarLayout( sidebarPanel( h3(\u0026quot;Data Selection\u0026quot;) ), mainPanel( h3(\u0026quot;Head of the Dataset\u0026quot;), h3(\u0026quot;Dataset Summary\u0026quot;) ) ) ), tabPanel(\u0026quot;Visual EDA - Time Series\u0026quot;, fluid = TRUE, #tab 2 sidebarLayout( sidebarPanel( h3(\u0026quot;Type of Graph\u0026quot;) ), mainPanel( h3(\u0026quot;Trendlines\u0026quot;) ) ) ), tabPanel(\u0026quot;Visual EDA - Discrete\u0026quot;, fluid = TRUE, #tab 3 sidebarLayout( sidebarPanel( h3(\u0026quot;Type of Graph\u0026quot;) ), mainPanel( h3(\u0026quot;Discrete Variable\u0026quot;) ) ) ), tabPanel(\u0026quot;Visual EDA - Continuous\u0026quot;, fluid = TRUE, #tab 4 sidebarLayout( sidebarPanel( h3(\u0026quot;Type of Graph\u0026quot;) ), mainPanel( h3(\u0026quot;Continuous Varible\u0026quot;) ) ) ) ) ) server = function(input, output) {} shinyApp(ui, server)  Next, we would want to define control widgets to feed our active inputs. To that end, we would want to use numericInput(), selectInput() and radioButtons(). A list of other control widgets can be found on the Shiny cheatsheet. We would also want to add submitButton() to prevent reaction on the entire app.\nui = fluidPage( titlePanel(\u0026quot;Gun-Related Deaths in the United States (2012-2014)\u0026quot;), tabsetPanel( tabPanel(\u0026quot;Quantitative EDA\u0026quot;, fluid = TRUE, sidebarLayout( sidebarPanel( h3(\u0026quot;Data Selection\u0026quot;), numericInput(inputId = \u0026quot;obs\u0026quot;, label = \u0026quot;Number of Observations:\u0026quot;, value = 3), #numeric input submitButton(text=\u0026quot;View EDA\u0026quot;) #submit button ), mainPanel( h3(\u0026quot;Head of the Dataset\u0026quot;), h3(\u0026quot;Dataset Summary\u0026quot;) ) ) ), tabPanel(\u0026quot;Visual EDA - Time Series\u0026quot;, fluid = TRUE, sidebarLayout( sidebarPanel( h3(\u0026quot;Type of Graph\u0026quot;), radioButtons(inputId = \u0026quot;intent\u0026quot;, label = \u0026quot;Intent:\u0026quot;, choices = levels(cases_new$intent), selected = \u0026quot;Homicide\u0026quot;), #checkbox selectInput(inputId = \u0026quot;trendline_color\u0026quot;, label = \u0026quot;Color:\u0026quot;, choices = colnames(cases_new)[c(5:6,8,10:11)], selected = \u0026quot;race\u0026quot;), #dropdown submitButton(text=\u0026quot;View EDA\u0026quot;) #submit button ), mainPanel( h3(\u0026quot;Trendlines\u0026quot;) ) ) ), tabPanel(\u0026quot;Visual EDA - Discrete\u0026quot;, fluid = TRUE, sidebarLayout( sidebarPanel( h3(\u0026quot;Type of Graph\u0026quot;), radioButtons(inputId = \u0026quot;variable\u0026quot;, label = \u0026quot;Variable:\u0026quot;, choices = colnames(cases_new)[c(4:6,8,10:11)], selected = \u0026quot;intent\u0026quot;), #checkbox selectInput(inputId = \u0026quot;discrete_color\u0026quot;, label = \u0026quot;Color:\u0026quot;, choices = colnames(cases_new)[c(4:6,8,10:11)], selected = \u0026quot;race\u0026quot;), #dropdown submitButton(text=\u0026quot;View EDA\u0026quot;) #submit button ), mainPanel( h3(\u0026quot;Discrete Variable\u0026quot;) ) ) ), tabPanel(\u0026quot;Visual EDA - Continuous\u0026quot;, fluid = TRUE, sidebarLayout( sidebarPanel( h3(\u0026quot;Type of Graph\u0026quot;), h5(\u0026quot;Independent Variable: Age\u0026quot;), radioButtons(inputId = \u0026quot;plot_type\u0026quot;, label = \u0026quot;Plot:\u0026quot;, choices = c(\u0026quot;density plot\u0026quot;,\u0026quot;box plot\u0026quot;), selected = \u0026quot;density plot\u0026quot;), #checkbox selectInput(inputId = \u0026quot;continuous_color\u0026quot;, label = \u0026quot;Color:\u0026quot;, choices = colnames(cases_new)[c(4:6,8,10:11)], selected = \u0026quot;race\u0026quot;), #dropdown submitButton(text=\u0026quot;View EDA\u0026quot;) #submit button ), mainPanel( h3(\u0026quot;Continuous Varible\u0026quot;) ) ) ) ) ) server = function(input, output) {} shinyApp(ui, server)  The inputID argument in each widget function will be used to call out our inputs when we feed them into the server.\nThen, we would want to define the types of output within each mainPanel() that we would want to show on the UI. The output on the UI will work hand-in-hand with the render*() functions in the server. Again, for a list of them, check the cheatsheet.\nui = fluidPage( titlePanel(\u0026quot;Gun-Related Deaths in the United States (2012-2014)\u0026quot;), tabsetPanel( tabPanel(\u0026quot;Quantitative EDA\u0026quot;, fluid = TRUE, sidebarLayout( sidebarPanel( h3(\u0026quot;Data Selection\u0026quot;), numericInput(inputId = \u0026quot;obs\u0026quot;, label = \u0026quot;Number of Observations:\u0026quot;, value = 3), submitButton(text=\u0026quot;View EDA\u0026quot;) ), mainPanel( h3(\u0026quot;Head of the Dataset\u0026quot;), tableOutput(\u0026quot;view\u0026quot;), #table output h3(\u0026quot;Dataset Summary\u0026quot;), verbatimTextOutput(\u0026quot;summary\u0026quot;) #text output ) ) ), tabPanel(\u0026quot;Visual EDA - Time Series\u0026quot;, fluid = TRUE, sidebarLayout( sidebarPanel( h3(\u0026quot;Type of Graph\u0026quot;), radioButtons(inputId = \u0026quot;intent\u0026quot;, label = \u0026quot;Intent:\u0026quot;, choices = levels(cases_new$intent), selected = \u0026quot;Homicide\u0026quot;), selectInput(inputId = \u0026quot;trendline_color\u0026quot;, label = \u0026quot;Color:\u0026quot;, choices = colnames(cases_new)[c(5:6,8,10:11)], selected = \u0026quot;race\u0026quot;), submitButton(text=\u0026quot;View EDA\u0026quot;) ), mainPanel( h3(\u0026quot;Trendlines\u0026quot;), plotOutput(\u0026quot;plot1\u0026quot;) #plot output ) ) ), tabPanel(\u0026quot;Visual EDA - Discrete\u0026quot;, fluid = TRUE, sidebarLayout( sidebarPanel( h3(\u0026quot;Type of Graph\u0026quot;), radioButtons(inputId = \u0026quot;variable\u0026quot;, label = \u0026quot;Variable:\u0026quot;, choices = colnames(cases_new)[c(4:6,8,10:11)], selected = \u0026quot;intent\u0026quot;), selectInput(inputId = \u0026quot;discrete_color\u0026quot;, label = \u0026quot;Color:\u0026quot;, choices = colnames(cases_new)[c(4:6,8,10:11)], selected = \u0026quot;race\u0026quot;), submitButton(text=\u0026quot;View EDA\u0026quot;) ), mainPanel( h3(\u0026quot;Discrete Variable\u0026quot;), plotOutput(\u0026quot;plot2\u0026quot;) #plot output ) ) ), tabPanel(\u0026quot;Visual EDA - Continuous\u0026quot;, fluid = TRUE, sidebarLayout( sidebarPanel( h3(\u0026quot;Type of Graph\u0026quot;), h5(\u0026quot;Independent Variable: Age\u0026quot;), radioButtons(inputId = \u0026quot;plot_type\u0026quot;, label = \u0026quot;Plot:\u0026quot;, choices = c(\u0026quot;density plot\u0026quot;,\u0026quot;box plot\u0026quot;), selected = \u0026quot;density plot\u0026quot;), selectInput(inputId = \u0026quot;continuous_color\u0026quot;, label = \u0026quot;Color:\u0026quot;, choices = colnames(cases_new)[c(4:6,8,10:11)], selected = \u0026quot;race\u0026quot;), submitButton(text=\u0026quot;View EDA\u0026quot;) ), mainPanel( h3(\u0026quot;Continuous Varible\u0026quot;), plotOutput(\u0026quot;plot3\u0026quot;) #plot output ) ) ) ) ) server = function(input, output) {} shinyApp(ui, server)  Step 2: Define a server. After all that is done, now on to the “easier” part. For our server, we require two basic things: a reactive source (which we would call as the input) and a reactive endpoint (which we would call as the output), to which we will attempt to connect the two of them. As an intermediary point, we would also define a reactive conductor using the reactive({}) call in shiny. Based on the previous input and output defined on our UI, we will update our server function with the following:\n(For a list of render*() functions, check cheatsheet.)\nui = fluidPage( titlePanel(\u0026quot;Gun-Related Deaths in the United States (2012-2014)\u0026quot;), tabsetPanel( tabPanel(\u0026quot;Quantitative EDA\u0026quot;, fluid = TRUE, sidebarLayout( sidebarPanel( h3(\u0026quot;Data Selection\u0026quot;), numericInput(inputId = \u0026quot;obs\u0026quot;, label = \u0026quot;Number of Observations:\u0026quot;, value = 3), submitButton(text=\u0026quot;View EDA\u0026quot;) ), mainPanel( h3(\u0026quot;Head of the Dataset\u0026quot;), tableOutput(\u0026quot;view\u0026quot;), h3(\u0026quot;Dataset Summary\u0026quot;), verbatimTextOutput(\u0026quot;summary\u0026quot;) ) ) ), tabPanel(\u0026quot;Visual EDA - Time Series\u0026quot;, fluid = TRUE, sidebarLayout( sidebarPanel( h3(\u0026quot;Type of Graph\u0026quot;), radioButtons(inputId = \u0026quot;intent\u0026quot;, label = \u0026quot;Intent:\u0026quot;, choices = levels(cases_new$intent), selected = \u0026quot;Homicide\u0026quot;), selectInput(inputId = \u0026quot;trendline_color\u0026quot;, label = \u0026quot;Color:\u0026quot;, choices = colnames(cases_new)[c(5:6,8,10:11)], selected = \u0026quot;race\u0026quot;), submitButton(text=\u0026quot;View EDA\u0026quot;) ), mainPanel( h3(\u0026quot;Trendlines\u0026quot;), plotOutput(\u0026quot;plot1\u0026quot;) ) ) ), tabPanel(\u0026quot;Visual EDA - Discrete\u0026quot;, fluid = TRUE, sidebarLayout( sidebarPanel( h3(\u0026quot;Type of Graph\u0026quot;), radioButtons(inputId = \u0026quot;variable\u0026quot;, label = \u0026quot;Variable:\u0026quot;, choices = colnames(cases_new)[c(4:6,8,10:11)], selected = \u0026quot;intent\u0026quot;), selectInput(inputId = \u0026quot;discrete_color\u0026quot;, label = \u0026quot;Color:\u0026quot;, choices = colnames(cases_new)[c(4:6,8,10:11)], selected = \u0026quot;race\u0026quot;), submitButton(text=\u0026quot;View EDA\u0026quot;) ), mainPanel( h3(\u0026quot;Discrete Variable\u0026quot;), plotOutput(\u0026quot;plot2\u0026quot;) ) ) ), tabPanel(\u0026quot;Visual EDA - Continuous\u0026quot;, fluid = TRUE, sidebarLayout( sidebarPanel( h3(\u0026quot;Type of Graph\u0026quot;), h5(\u0026quot;Independent Variable: Age\u0026quot;), radioButtons(inputId = \u0026quot;plot_type\u0026quot;, label = \u0026quot;Plot:\u0026quot;, choices = c(\u0026quot;density plot\u0026quot;,\u0026quot;box plot\u0026quot;), selected = \u0026quot;density plot\u0026quot;), selectInput(inputId = \u0026quot;continuous_color\u0026quot;, label = \u0026quot;Color:\u0026quot;, choices = colnames(cases_new)[c(4:6,8,10:11)], selected = \u0026quot;race\u0026quot;), submitButton(text=\u0026quot;View EDA\u0026quot;) ), mainPanel( h3(\u0026quot;Continuous Varible\u0026quot;), plotOutput(\u0026quot;plot3\u0026quot;) ) ) ) ) ) server = function(input, output) { active_dataset_trendline= reactive({ if(input$intent == \u0026quot;Homicide\u0026quot;) { count_transformation(homicide_cases, input$trendline_color) } else if (input$intent == \u0026quot;Suicide\u0026quot;) { count_transformation(suicide_cases, input$trendline_color) } else if(input$intent == \u0026quot;Accidental\u0026quot;){ count_transformation(accidental_cases, input$trendline_color) } else if (input$intent == \u0026quot;Undetermined\u0026quot;){ count_transformation(undetermined_cases, input$trendline_color) } }) #reactive conductor, i.e. transformed dataset active_graph= reactive({ if(input$plot_type == \u0026quot;density plot\u0026quot;) { density_plot(cases_new, \u0026quot;age\u0026quot;, color = input$continuous_color) + labs(title = paste(\u0026quot;Deaths: age by\u0026quot;,input$continuous_color)) } else if (input$plot_type == \u0026quot;box plot\u0026quot;) { box_plot(cases_new, input$continuous_color, \u0026quot;age\u0026quot;, color = input$continuous_color) + theme(axis.text.x = element_text(angle = 65, hjust = 1)) + labs(title = paste(\u0026quot;Deaths: age by\u0026quot;,input$continuous_color)) } }) #reactive conductor, i.e. desired output for tab 4. #Note~ a reactive conductor can be called again to act as an endpoint. output$view = renderTable({ head(case_transformation(cases_new),n = input$obs) }) output$summary = renderPrint({ summary(case_transformation(cases_new)) }) #output for tab 1 output$plot1 = renderPlot({ line_plot(active_dataset_trendline(), \u0026quot;date\u0026quot;, \u0026quot;Total_cases\u0026quot;, color = input$trendline_color) + labs(title = paste(\u0026quot;Deaths:\u0026quot;, input$intent,\u0026quot;by\u0026quot;,input$trendline_color), x = \u0026quot;Date\u0026quot;, y = \u0026quot;Total Cases\u0026quot;) }) #output for tab 2 output$plot2 = renderPlot({ bar_plot(cases_new, input$variable, fill = input$discrete_color, dodge = TRUE) + theme(axis.text.x = element_text(angle = 65, hjust = 1)) + labs(title = paste(\u0026quot;Deaths:\u0026quot;, input$variable,\u0026quot;by\u0026quot;,input$discrete_color)) }) #output for tab 3 output$plot3 = renderPlot({ active_graph() }) #output for tab 4 } shinyApp(ui, server) Now, we are basically done. Our app/dashboard is finished! We now henceforth have the choice to publish our app on a shinyapps.io live server for other people to use, like I have for mine.\n\nThe Final Product link to app:\n You could also follow the same logic to give your dashboard a more crisp look by downloading the themes from the shinydashboard package available on CRAN.\n Small (perhaps Related) Discussion Now, to us statisticians (or data scientists), why does this matter? Why went through such lengths to produce this type of result? When I first took a programming class with coatless, I asked myself the same question too, to realize later how important it was as I took other statistics classes.\nWithout reservation, one rarely doubts the mental acuity of a statistician to produce meaningful work. However, in some cases, even the prolific ones aren’t the best at communicating their results, which is why having tools such as this is so important. Growing up, I used to be a big fan of Richard Feynman, the physicist known for his “eccentric” (or rather, unique) way of explaining difficult concepts in physics. I often listened to his videos while doing my math homeworks, which not only did it supply me with the motivation, but also imparted a philosophy within me that (truthfully) not many people have—that is, to explain with clarity and brevity so much so even the most distant layman could understand. And that that sometimes requires intellectual humility and empathy to put our ourselves in others’ shoes.\nSo, communicate with the intent to be understood.\n ","date":1546819200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546819200,"objectID":"c8337a4ec8fe5796767f25a4503ed343","permalink":"/post/getting-interactive-with-shiny/getting-interactive-with-shiny/","publishdate":"2019-01-07T00:00:00Z","relpermalink":"/post/getting-interactive-with-shiny/getting-interactive-with-shiny/","section":"post","summary":"The greatest value of a picture is when it forces us to notice what we never expected to see.","tags":["Shiny"],"title":"Getting Interactive with Shiny","type":"post"},{"authors":null,"categories":["R"],"content":" Microblogging is becoming more and more ubiquitous among today’s generation. In fact, interactions on some type of online platform or service such as Facebook, Twitter or Google leave traces of data that show a record of behavior or actions. As Davidowitz put it in his book Everybody Lies: “The everyday act of typing a word or phrase into a compact, rectangular white box leaves a small trace of truth that, when multiplied by millions, eventually reveals profound realities.” To statisticians, this quote faintly reminds of the many statistical ideas that could be put to the test.\nAn emerging role of online platforms has been in the political context. As such, our research interest would be to predict political outcomes with a preferred dataset that is in line with our hypothesis and allows us to scientifically control for many variables.\n Hypothesis: We could use sentiments on social media to predict public opinions as a proxy for political outcomes.\n The scope of the preferred dataset(s) that we are looking into include observations\n whose geolocations match those of the political outcomes of interest. represent the voter base. And; are longitudinal.  To that end, we gathered the following dataset from Kaggle:\nHillary Clinton and Donald Trump Tweets   Features Description    handle Twitter handle name  text Tweets  is_retweet Whether the tweet was retweeted  original_author Original author  time Timestamp  in_reply_to_screen_name -  in_reply_to_status_id -  in_reply_to_user_id -  is_quote_status Whether the tweet was quoted  lang Twitter’s guess at language  retweet_count Retweet count  favorite_count Favorite count  longitude Longitude  latitude Latitude  place_id Place id  place_full_name Place full name  place_name Place name  place_type Place type  place_country_code Country code  place_country Country  place_contained_within Place contained within  place_attributes Place attributes  place_bounding_box Place bounding box  source_url Tweet source url  truncated Whether it is truncated  entities a JSON object  extended_entities Another JSON object    The dimensions of the dataset:\n## [1] 7468 3 First five observations of both candidates:\n@HillaryClinton:\n## text ## 1 Thank you @_VicenteFdez. You\u0026#39;re right—\u0026quot;su voz es su voto.\u0026quot; So grateful to have your support. #JuntosSePuede https://t.co/j9v2K84AV1 ## 2 No matter where you live you can make sure you\u0026#39;re registered to vote at https://t.co/tTgeqxNqYm.… https://t.co/tWlu8DiGIg ## 3 RT @azcentral: The Arizona Republic ed board has endorsed @HillaryClinton. See why: https://t.co/FqStTwkesL via @azcopinions #elections2016 ## 4 20 years after Donald bullied a beauty pageant winner for her weight the real \u0026quot;problem\u0026quot; is...still Donald. https://t.co/ZmqYWuN9px ## 5 The question in this election: Who can put the plans into action that will make your life better? https://t.co/XreEY9OicG ## time handle ## 1 09-28-2016 02:36:13 HillaryClinton ## 2 09-28-2016 02:10:42 HillaryClinton ## 3 09-28-2016 01:19:46 HillaryClinton ## 4 09-28-2016 01:13:08 HillaryClinton ## 5 09-28-2016 00:22:34 HillaryClinton @realDonaldTrump:\n## text ## 4191 My supporters are the best! $18 million from hard-working people who KNOW what we can be again! Shatter the record: https://t.co/8ZHGyOth0f ## 4192 Unbelievable evening in Melbourne Florida w/ 15000 supporters- and an additional 12000 who could not get in. Tha… https://t.co/VU5wh2zXBU ## 4193 Join me for a 3pm rally - tomorrow at the Mid-America Center in Council Bluffs Iowa! Tickets:… https://t.co/dfzsbICiXc ## 4194 Once again we will have a government of by and for the people. Join the MOVEMENT today! https://t.co/lWjYDbPHav https://t.co/uYwJrtZkAe ## 4195 RT @GOP: On National #VoterRegistrationDay make sure you\u0026#39;re registered to vote so we can #MakeAmericaGreatAgain https://t.co/GKcaLkx8C8 ht… ## time handle ## 4191 09-28-2016 02:17:06 realDonaldTrump ## 4192 09-28-2016 01:03:03 realDonaldTrump ## 4193 09-27-2016 22:13:24 realDonaldTrump ## 4194 09-27-2016 21:08:22 realDonaldTrump ## 4195 09-27-2016 20:31:14 realDonaldTrump According to Google Trends, the keywords most searched on Google during the 2016 election are “Abortion”, “Immigration”, “Race Issues”, “Economy”, “Affordable Care Act”, “ISIS”, “Climate Change”, “National Debt”, “Gun Control” and “Voting System.” Using gtrendsR, we randomly picked five of those keywords and queried its search hits by setting geo = \u0026quot;US\u0026quot; to prevent bias.\ntrend1 = gtrends(c(\u0026quot;immigration\u0026quot;, \u0026quot;abortion\u0026quot;, \u0026quot;economy\u0026quot;, \u0026quot;gun control\u0026quot;, \u0026quot;terrorism\u0026quot;), geo = \u0026quot;US\u0026quot;, time = \u0026quot;2016-01-01 2016-12-31\u0026quot;) plot(trend1) Lo and behold, the trends do not differ much when we queried for Trump and Hillary.\ntrend2 = gtrends(c(\u0026quot;hillary\u0026quot;, \u0026quot;trump\u0026quot;), geo = \u0026quot;US\u0026quot;, time = \u0026quot;2016-01-01 2016-12-31\u0026quot;) plot(trend2) The trends show the relative popularity of the search query adjusted by the total number searches and time period. We see that these trends look more similar around mid-campaign.\nAnother popular measure of public opinions are by using polls. To that end, we queried HuffPost Pollster, a poll that aggregates every poll that claims to represent the population using pollstR.\nWe obtained the following charts using the slug 2016-general-election-trump-vs-clinton with pollstR:\ntrend.plot We hypothesize that these trends are invoked by the use of sentiments from both political candidates during their campaigns either through speech or social media. Using the trends as the labels and the sentiments as the features, we could attempt to build a model to find out their relationship.\n Literature Review  If I have seen further it is by standing on the shoulders of Giants.\n— Isaac Newton\n Indeed, the Google search engine is seen as this black box that confounds from the feeblest to the strongest of men. Only until recent years that it is seen as a treasure trove to people who analyze big data. Seth Stephen-Davidowitz, a former data scientist at Google, addressed in his book Everybody Lies: Big Data, New Data and What the Internet Can Tell Us that\n … there’s something very comforting about that little white box that people feel very comfortable telling things that they may not tell anybody else about: Their sexual interests, their health problems, their insecurities. And using this anonymous aggregate data we can learn a lot more about people than we’ve really ever known.\n This reveals an implicit truth: that these data probably understand people more than they do about themselves. Thus, on account to this, many have studied google search queries in hopes to uncover this social behavior. As the Japanese say there are three faces of self: one you show it to the world, the second you show to your close friends, and the third you don’t show it to anyone. For whatever reason, people seem to comfortable with telling everything to the white little box on a daily basis.\nAnother popular metric to “quantify” this social behavior is by using polls that are collected by surveyors. These polls are taken and studied insofar as they represent the population, as written by Huffpost Pollster. The survey questions on the polls are designed so much so that that they are unbiased so the survey takers would be able to answer them impartially. However, over the recent years, there have been counless debates against the credibility of the polls.\nAs evident during the 2016 US election, Hillary Clinton led the popular votes by a huge margin and that the polls predicted that she would be the one to take oath of office at the beginning of the next term. However, the contrary happened and that our reliance on predictive analytics on reams of data had outstripped the understanding of its limitations: that they are tools of probabilistic estimate, but still quite useful. A paper written by Kassraie, Modirshanechi \u0026amp; Aghajan (2017) attempted to do just that. The authors collected tweets from the public during the election and aggregated their sentiments so that they could be used to perform some predictive evaluation onto weekly election polls. They fitted a linear model by first choosing an uncorrelated set of features that would make the regression’s dimensions perpendicular. The poll ratings are then used as the response. The results are shown on these graphs:\nFigure 1: Predicting online election polls with sentiment data for Clinton.\n Figure 2: Predicting online election polls with sentiment data for Trump.\n As far as prediction goes, using sentiments on social media to predict election polls can be seen as a viable choice given the above result. Of the two predictions, the model on Clinton achieved a mean error of 0.50 % and that on Trump achieved a mean error of 1.08 %.\nAs alluded previously, prediction using polls as a means of predicting political outcomes has received many criticisms. One of the criticisms allegedly claimed that polls failed to capture the caprice of voters. For example, a Trump loyalist may vote in favor of the opposing party on the online polls but might have behaved otherwise when they cast ballot. For this reason and more, we will attempt to predict social behavior by using as Google Trends with our Twitter dataset.\n Approach/ Design/ Analysis Sentiment Analysis Before performing any ML algorithm on our dataset, let us take a peek into it by performing some sentiment analysis. After “cleaning” the tweets by removing the links, special characters, and stop words, we could obtain the following plots:\nThis plot shows the number of sentiments used by each candidate over time. Note that this sentiment analysis is based on the frequency at which the word appear on the Twitter dataset. From the plot, we could generally see that one of the candidate’s sentiments are more geared towards the negative, while the other seems remain stable around the positive side fof the rhetoric.\nNow, from this plot, we can see the words that are contributing to the sentiments from the most positive to the most negative. Here, we chose the words that appear at least 35 times for better interpretation. Knowing this information is useful in designing and validating our hypothesis.\n Prediction Define a popularity vector \\(\\mathbf{P}\\) as\n\\[\\mathbf{P} = \\frac{|\\{w_i : w_i \\in W\\} |_t}{|W|_t} \\in \\{[0, 1]\\}_{t=1}^n\\]\n, which is approximately how the Google Trend’s search hits are defined after adjusting for time. We want to estimate \\(\\mathbf{\\hat{P}}\\) using the sentiments to predict social behavior.\nTo that end, we would like to design a document-term matrix by keeping only the sentiments and and then fit several models to estimate the Google Trends.\n1) Cleaning the data We are mostly interested in the tweets of the dataset and their timestamps, hence we would like to clean them so that we will only get the sentiments to obtain the document-term matrix. We will use the tm package to achieve that.\nThe following code is used to obtain the document-term matrix:\n# Obtaining a corpus object corpus = as.character(tweets_mutated[, \u0026quot;tweets\u0026quot;]) %\u0026gt;% removeURL %\u0026gt;% removeSpecial %\u0026gt;% stemWord %\u0026gt;% VectorSource %\u0026gt;% VCorpus # Transforming the corpus cleaned_corpus = tm_map(corpus, tolower) %\u0026gt;% tm_map(PlainTextDocument) %\u0026gt;% tm_map(removePunctuation) %\u0026gt;% tm_map(removeNumbers) %\u0026gt;% tm_map(removeWords, stopwords(\u0026quot;english\u0026quot;)) %\u0026gt;% tm_map(stripWhitespace) # Obtaining the Document Term Matrix DTM = DocumentTermMatrix(cleaned_corpus) %\u0026gt;% removeSparseTerms(0.99) %\u0026gt;% as.matrix # Merging the data res_H = cbind.data.frame(DTM, \u0026quot;hits\u0026quot; = tweets_mutated$hillary_hits/100 ) res_T = cbind.data.frame(DTM, \u0026quot;hits\u0026quot; = tweets_mutated$trump_hits/100 ) The idea is to pipeline the tweets from the dataset through a process of transformations. An initial glimpse of the data reveals that the some of the tweets have links and special characters (such as emojis) that needed to be removed, hence we created the functions removeURL and removeSpecial to do just that. Furthermore, we recognized that, depending on the context, some words mean the same thing, hence we had to “stem” the words so that they will return to their original word. A function called hunspell_stem() from the hunspell package allowed us to do just that (and more efficiently than a similar from the tm package). The function is included in the functional stemWord so that the tweets that go through it will return the word in its root form.\nSuppose a vector of characters:\n[1] \u0026quot;loving\u0026quot; \u0026quot;love\u0026quot; \u0026quot;loved\u0026quot; \u0026quot;lovely\u0026quot;  Depending on the context, we would want a vector that would be expressed as following:\n[1] \u0026quot;love\u0026quot; \u0026quot;love\u0026quot; \u0026quot;loved\u0026quot; \u0026quot;love\u0026quot;  This process is important so that the document term matrix can pick up on the frequency at which the sentiments appear since it usually returns a matrix that is sparse and that that would be problematic when interpreting the results. Furthermore, we let the tweets ran through some more functions like tolower,removePunctuation, removeNumbers, removeWords (to remove English stopwords) and stripWhiteSpace to obtain tweets of only sentiments.\nNote that before we had to go through this process, we had to concatenate the strings by each day to match with the number of hits we have since we have too have them by day. This is in line with our hypothesis that the search hits are not only affected by the sentiments invoked by one candidate individually, but rather collectively.\nAfter transforming the tweets, we transformed them into a document-term matrix and remove sparse terms.\nThe dimension of our document-term matrix is:\ndim(DTM) ## [1] 268 226 And now, we are ready to do our analysis.\n 2) Logistic Regression With the dataset, we attempt to fit a logistic regression since our relative popularity (i.e. search hits) is designed to be bounded from 0 to 1 and signify proportions. First, we will crudely fit the response with all of the features. Once we do that, we will get the following results:\nHillary’s search hits logistic diagnostic:  Trump’s search hits logistic diagnostic: If we look at the diagnostic, we see that none of the assumptions for inference are violated, so we could easily perform the usual hypothesis testings. From the plots, we see that we almost perfectly fit the trends with the given design matrix. For both of the models, we obtain the following Mean Squared Error (MSE):\n# MSE for Hillary\u0026#39;s model (adjusted for scale) mse(logit_H$fitted.values,res_H$hits)*100^2 ## [1] 1.043019 # MSE for Trump\u0026#39;s model (adjusted for scale) mse(logit_T$fitted.values,res_T$hits)*100^2 ## [1] 22.39258 However, if we care about parsimony, we may wish to perform some variable selection with AIC since AIC generally works well with prediction and that it penalizes much less. To that end, we will use the stepAIC() function:\n# Number of features used ## On Hillary\u0026#39;s GTrends length(AIC.glm_H$coefficients) - 1  ## [1] 226 ## On Trump\u0026#39;s GTrends length(AIC.glm_T$coefficients) - 1  ## [1] 226 We see that none of the variables were penalized, hence the MSEs for both models remain the same. This variable selection method does work that well because the directional variable selection has a one-way solution path thus it may not exhaust all possible subset selection.\n  3) LASSO vs Ridge Regressions We may also be interested in doing a penalized linear regressions to find out which variables matter more in the estimate. Two of the most common penalized regressions are the LASSO and Ridge regressions, with the former penalizes the coefficients more. Since our design matrix is a document-term matrix, some terms may not be that useful, hence using the penalized regressions can help us ignore those variables. We attempt to tune the penalty for LASSO first:\n# LASSO fit for Hillary cvH.fit=cv.glmnet(as.matrix(res_H[,1:227]),as.matrix(res_H$hits),type.measure=\u0026quot;mse\u0026quot;) plot(cvH.fit) # LASSO fit for Trump cvT.fit=cv.glmnet(as.matrix(res_T[,1:227]),as.matrix(res_T$hits),type.measure=\u0026quot;mse\u0026quot;) plot(cvT.fit) # The tuning parameters ## For Hillary cvH.fit$lambda.1se ## [1] 0.001972102 ## For Trump cvT.fit$lambda.1se ## [1] 0.0046166 The parameters chosen here are the ones that minimize the MSE, hence we will be using them as the parameter to fit our models.\n## MSE for Hillary\u0026#39;s LASSO model (adjusted for scale) mse(yhat111,res_H$hits)*100^2 ## [1] 0.03889185 ## MSE for Trump\u0026#39;s LASSO model (adjusted for scale) mse(yhat222,res_T$hits)*100^2 ## [1] 0.21313 We see that the MSE for both of the plots seem to fit perfectly, which is a concern since we have a sparse matrix. Due to this, performing a gradient descent will not be meaningful.\nFuthermore, we may now wish to perform a Ridge regression.\n# Define a sequence of lambdas l = seq(0, 1000, by = 0.01) # Fitting the models with lambdas ridge.H = lm.ridge(hits~., res_H, lambda = l) ridge.T = lm.ridge(hits~., res_T, lambda = l) # Plotting GCV against lambdas plot(ridge.H$lambda, ridge.H$GCV, col = \u0026quot;darkorange\u0026quot; ,type = \u0026quot;l\u0026quot;, ylab = \u0026quot;GCV\u0026quot;, xlab = \u0026quot;lambda\u0026quot;, main = \u0026quot;GCV against lambdas for Hillary\u0026#39;s Ridge Regression\u0026quot;) plot(ridge.T$lambda, ridge.T$GCV, col = \u0026quot;darkorange\u0026quot; ,type = \u0026quot;l\u0026quot;, ylab = \u0026quot;GCV\u0026quot;, xlab = \u0026quot;lambda\u0026quot;, main = \u0026quot;GCV against lambdas for Trump\u0026#39;s Ridge Regression\u0026quot;) Similar to LASSO, the Ridge regression has the same tuning parameter for penalty, which we will tune differently than LASSO by using the GCV metric. After obtaining the minimum GCV that corresponds with the lambdas, we will then use that lambda to fit our models.\nThe following are the MSE’s for both models:\n## MSE for Hillary\u0026#39;s Ridge model (adjusted for scale) mse(y.pred.H,res_H$hits)*100^2 ## [1] 14.93831 ## MSE for Trump\u0026#39;s Ridge model (adjusted for scale) mse(y.pred.T, res_T$hits)*100^2 ## [1] 130.2933 We see that Ridge regressions perform badly for Trump’s Google Trends hits relative to that of Hillary. We say that this might be due to the sharp peaks that the Trump’s hits may have, or that maybe it is due to the dataset being collected since Trump’s tweets are fewer than Hillary’s.\n 4) Random Forest With Random forest we first try the default version, and realize that by simply doing randomForest() and setting up the ntree we still have a pretty bad model. Therefore we start tunning the parameters of mtry and ‘nodesize’:\nWe can see that Clinton ’s optimal mtry is 80 and optimal nodesize is 32. Trump’s optimal mtry is 77 and its optimal nodesize is 37.\nFrom the variable importance plots of both models, the term candidates seem to show up the most in both.\nThe result of the Mean Squared Errors are listed as follows:\n## MSE for Hillary\u0026#39;s RF model (adjusted for scale) mse(rf.H$predicted, res_H$hits)*100^2 ## [1] 35.92742 ## MSE for Trump\u0026#39;s RF model (adjusted for scale) mse(rf.T$predicted, res_T$hits)*100^2 ## [1] 224.5075  5) Feature Selection We could attempt to find the features that greatly affect the prediction. However, since we have features of only factor variables, computing the distance matrices may be a problem. As such, we could use the FactoMineR package to solve this problem since it would allow us to compute the principal components for mixed variables\nfact = res_H[,1:226] %\u0026gt;% lapply(as.factor) %\u0026gt;% do.call(cbind, .) res.pca=PCA(fact[,1:226]) We see that doing a feature selection by PCA from each of the plots since they did not contribute too much to the explanations of the variance for both dimensions. Therefore, we say that using PCA on a document-term matrix may not work that well since our variables are not continuous.\n  Discussion From the results, we see that the outcome of our prediction vary a lot based on the characteristics of the methods we applied and how certain methods are impacted by the data sets.\nFor the logistic model, by applying AIC for variable selection, we see that none of the variables were removed. This method may not work that well since the solution path may be one-way only, not exhaustive. Another method that we could have done is to use the best subset selection, which does an exhaustive search on all features to find the best subset of features that would fit the model best.\nFor the penalized regression models, we see that we encounter a problem with overfitting for the LASSO regression model. This may be due to the document term matrix being sparse, so LASSO had to force its ranks on all the covariates. Although we fit an almost perfect linear model with substantially low MSE, this may not be desirable if we take into account parsimony and the bias-variance tradeoff. For the Ridge regression, we found out that the model on Trump’s search hits did not work that well. If we examine the comparison plots closely, we found that the predicted values did not fit well when the trend had some sharp peaks. Furthermore, this may be due to the fact that Trump’s tweets are fewer than Hillary’s, which may have caused the fitted model to work better using Hillary’s search hits than Trump’s.\nUsing the method of Random Forest, we found that the models did not work that well, even after the we tune for mtry around p/3 and various nodesize. The variable importance also here used IncNodePurity, which may be the default for sparse document term matrix. We also realized that the randomForest() did not list the coefficients for our model, which may also be caused by the model matrix being sparse.\nTo summarize the methods, we computed the MSEs for each method to compare and contrast:\n  Method Trump Hillary    Logistic 22.43 1.05  LASSO 0.21 0.04  Ridge 130.35 15.09  Random Forest 227.79 36.75    We see that among the fitted models, the LASSO regression works the best in terms of Mean Squared Errors, while Random Forest performed the worst even after tuning the parameters mtry and nodesize. To present a visual result, we produced the following plots:\nH.comparison.plot T.comparison.plot We can see from the visualization which methods follow the patterns well.\nFinally, for the feature selection, we used the PCA method to find the features that would explain the variation the most. Given that we have columns of almost binary values, PCA would not work that well since it works best for continous variables. As can be seen from the PCA variance plot, only 7.4% of variation is explained by the terms in the document-term matrix. Another method that could have been done is by using Non-Negative Matrix Factorization (NMF), which may work well on a document-term matrix whose sentiments express themselves well in terms of frequencies at each document.\n Conclusion In conclusion, we can able to say that the sentiments on social media can predict public opinions as a proxy for political outcomes. Our potential pitfalls might be that the number of characters are pretty small for the data set by looking at nchar(), resulting in fewer sentiments for our analysis. If we are doing similar analysis in the future, we might choose data set with greater density for both candidates of the prediction and perform classification to the tweets into any class we are interested in if we have more time.\n Reference Kassraie, Modirshanechi \u0026amp; Aghajan. (2017). “Election Vote Share Prediction using a Sentiment-based Fusion of Twitter Data with Google Trends and Online Polls.” Scite Press, http://www.scitepress.org/Papers/2017/64843/64843.pdf\nStephens-Davidowitz, Seth. (2017). “Everybody Lies: Big Data, New Data, and What the Internet Can Tell Us About Who We Really Are.” The B\u0026amp;N Sci-Fi and Fantasy Blog, Barnes \u0026amp; Noble Reads, https://www.barnesandnoble.com/w/everybody-lies-seth-stephens-davidowitz/1125687298\n  ","date":1546732800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546732800,"objectID":"db8e8ae2034295e90d670959ed8d9474","permalink":"/project/2016-election-ml/sentiment-analysis-on-political-tweets-and-predicting-google-trends-search-queries/","publishdate":"2019-01-06T00:00:00Z","relpermalink":"/project/2016-election-ml/sentiment-analysis-on-political-tweets-and-predicting-google-trends-search-queries/","section":"project","summary":"Could we predict political outcomes with sentiments?","tags":["Sentiment Analysis","Supervised Learning"],"title":"Sentiment Analysis on Political Tweets and Predicting Google Trends Search Queries","type":"project"},{"authors":null,"categories":null,"content":" In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 \u0026hellip;\nTip 2 \u0026hellip;\n","date":1536469200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1536469200,"objectID":"6a451186c775f5f0adb3a0416d0cb711","permalink":"/tutorial/example/","publishdate":"2018-09-09T00:00:00-05:00","relpermalink":"/tutorial/example/","section":"tutorial","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 \u0026hellip;\nTip 2 \u0026hellip;","tags":null,"title":"Example Page","type":"docs"},{"authors":[],"categories":null,"content":"Click on the Slides button above to view the built-in slides feature.\n Slides can be added in a few ways:\n Create slides using Academic\u0026rsquo;s Slides feature and link using url_slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further talk details can easily be added to this page using Markdown and $\\rm \\LaTeX$ math code.\n","date":1483250400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483250400,"objectID":"96344c08df50a1b693cc40432115cbe3","permalink":"/talk/example/","publishdate":"2017-01-01T00:00:00-06:00","relpermalink":"/talk/example/","section":"talk","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam.","tags":[],"title":"Example Talk","type":"talk"},{"authors":["GA Cushen"],"categories":null,"content":"More detail can easily be written here using Markdown and $\\rm \\LaTeX$ math code.\n","date":1441083600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1441083600,"objectID":"e969bb54de9d92801015d349bd9d8b16","permalink":"/software/person-re-id/","publishdate":"2015-09-01T00:00:00-05:00","relpermalink":"/software/person-re-id/","section":"software","summary":"Person re-identification is a critical security task for recognizing a person across spatially disjoint sensors. Previous work can be computationally intensive and is mainly based on low-level cues extracted from RGB data and implemented on a PC for a fixed sensor network (such as traditional CCTV). We present a practical and efficient framework for mobile devices (such as smart phones and robots) where high-level semantic soft biometrics are extracted from RGB and depth data. By combining these cues, our approach attempts to provide robustness to noise, illumination, and minor variations in clothing. This mobile approach may be particularly useful for the identification of persons in areas ill-served by fixed sensors or for tasks where the sensor position and direction need to dynamically adapt to a target. Results on the BIWI dataset are preliminary but encouraging. Further evaluation and demonstration of the system will be available on our website.","tags":[],"title":"A Person Re-Identification System For Mobile Devices","type":"software"},{"authors":["GA Cushen","MS Nixon"],"categories":null,"content":"More detail can easily be written here using Markdown and $\\rm \\LaTeX$ math code.\n","date":1372654800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1372654800,"objectID":"52cea49c1c0499fc37b59639277321cb","permalink":"/software/clothing-search/","publishdate":"2013-07-01T00:00:00-05:00","relpermalink":"/software/clothing-search/","section":"software","summary":"We present a mobile visual clothing search system whereby a smart phone user can either choose a social networking photo or take a new photo of a person wearing clothing of interest and search for similar clothing in a retail database. From the query image, the person is detected, clothing is segmented, and clothing features are extracted and quantized. The information is sent from the phone client to a server, where the feature vector of the query image is used to retrieve similar clothing products from online databases. The phone's GPS location is used to re-rank results by retail store location. State of the art work focuses primarily on the recognition of a diverse range of clothing offline and pays little attention to practical applications. Evaluated on a challenging dataset, the system is relatively fast and achieves promising results.","tags":[],"title":"Mobile visual clothing search","type":"software"},{"authors":null,"categories":null,"content":" i\nSounds of needle leaves\nbristle through my ears over\nthe sunbled streamlet\nii\nClinking of pennies\nat an affluent building,\na bedridden pauper\niii\nSwans in ballet shoes\nunder the spotlit bedlam\ntip toe, tippy toe\niv\nA single mother\nAnd in her sinewy arms\nrests a smiling child\nv\nRobins chirruping\nunder the blanket of clouds\nnear the verdant meadow\nvi\nCity lights and roads\nstand witness to waste flyers\nand drunken high proles\nvii\nNear the dark alleys\nI heard noises not acute\nyet certainly trenchant\nviii\nThe sound of water\ntrickle is just as clear as\nthe rushing river\nix\nFour hundred twenty\nis the sigil of despair\nnot of alcohol\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"2fb56776273add7cfc09cb9af5c7a8c7","permalink":"/haikus/1/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/haikus/1/","section":"haikus","summary":"i\nSounds of needle leaves\nbristle through my ears over\nthe sunbled streamlet\nii\nClinking of pennies\nat an affluent building,\na bedridden pauper\niii\nSwans in ballet shoes\nunder the spotlit bedlam\ntip toe, tippy toe\niv\nA single mother\nAnd in her sinewy arms\nrests a smiling child\nv\nRobins chirruping\nunder the blanket of clouds\nnear the verdant meadow\nvi\nCity lights and roads\nstand witness to waste flyers","tags":null,"title":"","type":"haikus"},{"authors":null,"categories":null,"content":" Welcome to Slides Academic\nFeatures  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides  Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E  Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)  Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = \\;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \nA fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears  Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view   Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links   night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links  Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/img/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}  Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }  Questions? Ask\nDocumentation\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c2915ec5da95791851caafdcba9664af","permalink":"/slides/example-slides/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/slides/example-slides/","section":"slides","summary":"Welcome to Slides Academic\nFeatures  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides  Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E  Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)  Math In-line math: $x + y = z$","tags":null,"title":"Slides","type":"slides"}]